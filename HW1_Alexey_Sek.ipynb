{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание №1: линейная регрессия и векторное дифференцирование (10 баллов).\n",
    "\n",
    "* Максимальное количество баллов за задания в ноутбуке - 11, но больше 10 оценка не ставится, поэтому для получения максимальной оценки можно сделать не все задания.\n",
    "\n",
    "* Некоторые задания будут по вариантам (всего 4 варианта). Чтобы выяснить свой вариант, посчитайте количество букв в своей фамилии, возьмете остаток от деления на 4 и прибавьте 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Вариант\n",
    "\n",
    "len('Сек') % 4 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Многомерная линейная регрессия из sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим многомерную регрессию из sklearn для стандартного датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 100) (10000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples = 10000)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас 10000 объектов и 100 признаков. Для начала решим задачу аналитически \"из коробки\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4733280321693085e-25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "reg = LinearRegression().fit(X, y)\n",
    "print(mean_squared_error(y, reg.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем обучить линейную регрессию методом градиентного спуска \"из коробки\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8178026310899915e-12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.50172120e-08,  7.30179686e-09,  2.76451015e-08,  4.17150489e-08,\n",
       "       -2.59159314e-08, -1.42207128e-09,  1.09796338e-08,  8.93078959e+01,\n",
       "       -2.97181114e-08,  1.52891166e-08,  3.98966846e-08,  3.73823447e-08,\n",
       "        2.15398925e-08,  3.54150974e-08,  1.91629716e-10,  1.66049726e-08,\n",
       "       -1.15313012e-08, -2.41758070e-08,  3.43031208e-08,  7.94576254e-08,\n",
       "        3.40910477e+01,  2.88356528e+01, -1.77722224e-08,  3.41404640e-08,\n",
       "       -3.01098008e-09,  4.17853711e-08, -7.24611546e-08, -5.79103231e-09,\n",
       "       -8.94900562e-09,  2.56378035e-08,  8.11098268e-08, -9.96734504e-09,\n",
       "        2.91560773e-08,  1.52268432e-08,  1.06144532e-08, -1.32959310e-09,\n",
       "        1.69004412e-08, -2.00360854e-08, -3.02836042e-08, -3.16503117e-08,\n",
       "       -8.47234245e-08, -4.26993343e-09, -6.49913832e-09,  5.93787253e-08,\n",
       "        5.25474384e-08,  2.48623357e-09,  4.04265830e-08, -4.96889900e-08,\n",
       "       -3.30864395e-09,  9.49693916e+01,  8.00371100e-09,  5.75605024e+01,\n",
       "       -2.75384742e-09,  9.69152003e-09, -3.54013590e-08,  8.66931458e-09,\n",
       "        4.54836814e-08, -2.34897115e-08,  5.08826060e-08,  5.39397382e-08,\n",
       "        7.22827542e-09,  8.49700973e-09, -7.63859381e-09, -4.29340748e-08,\n",
       "       -3.26981379e-08,  4.31872309e-08,  2.38703101e-08,  2.73616661e-08,\n",
       "       -1.73802210e-09,  3.92077700e-08,  3.28163100e-09,  1.20788088e-08,\n",
       "        5.05751710e-08, -3.07763280e-09, -3.02576647e-08, -1.95235143e-08,\n",
       "        1.45485127e+01, -4.25405021e-08,  2.19391503e-08, -5.01977887e-09,\n",
       "        1.66555713e-08, -5.43568276e-09, -3.72050003e-08,  3.00839489e+00,\n",
       "        3.16981348e+01,  2.70767630e-08,  4.90886584e-08,  6.77172432e+01,\n",
       "        3.05228762e+01,  3.88191684e-08,  2.24289023e-08, -4.20665889e-09,\n",
       "        3.25900776e-08,  1.36262835e-08, -1.40396407e-09,  2.01966288e-08,\n",
       "       -2.01907241e-11,  1.19391881e-08,  1.94017860e-08, -2.06273764e-09])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "reg = SGDRegressor(alpha=0.00000001).fit(X, y)\n",
    "print(mean_squared_error(y, reg.predict(X)))\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 1 (0.5 балла).*** Объясните, чем вызвано различие двух полученных значений метрики?\n",
    "\n",
    "***Задание 2 (0.5 балла).*** Подберите гиперпараметры в методе градиентного спуска так, чтобы значение MSE было близко к значению MSE, полученному при обучении LinearRegression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1 - решение\n",
    "\n",
    "Разница в метрике MSE, так же, как и разница в коэффициентах получилась из-за того, что градиентный спуск - это стохастический способ нахождения оптмума и, в нем есть некоторая случайность, он может не совпадать с истинным (аналитическим) решением задачи.\n",
    "\n",
    "Случайность заключается в том, что в этом методе мы постепенно двигаемся шаг за шагом в сторону антиградиента, при этом заданный шаг или по-другому learning rate (гиперпараметр) сильно влияет на решение: слишком большой learning rate $\\Rightarrow$ высока вероятность \"пропустить\" минимум функции, слишком маленький learning rate $\\Rightarrow$ алгоритм будет сходиться очень долго, если вовсе сойдется\n",
    "\n",
    "Когда гиперпармаетры подобраны хорошо, стохастически найденное решение должно быть близко к аналитическому, но в этом случае разница кажется существенной, поэтому перейдем к заданию 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2 - решение\n",
    "\n",
    "Идея в том, чтобы кроссвалидацией подобрать оптимальное значение гиперпараметра alpha\n",
    "\n",
    "Чтобы получить минимальное различие MSE с аналитическим решением - я выбрал параметр scoring = 'neg_mean_squared_error'\n",
    "\n",
    "- В linspace немного отступил от нуля, потому что оптимизатор \"застревал\" в нуле и показывал, что оптимальная альфа равна нулю\n",
    "- Сначала я взял модель со всеми дефолтными параметрами, обучил ее и потом начал оптимизировать MSE по alpha кроссвалидацией\n",
    "- Получил оптимальную alpha, вставил этот оптимальный гиперпараметр в SGDRegressor, сделал predict и получил MSE\n",
    "- Полученный MSE методом градиентного бустинга совпадает порядок (e-25), что показывает, что у нас очень близкие MSE к аналитическому решению\n",
    "\n",
    "Если запускать код несколько раз, использую полученный оптимальный гиперпараметр - MSE будет изменяться, т.к. метод стохастический, а не аналитический, но при этом порядок MSE всегда остается таким же, как и в аналитическом методе (e-25)\n",
    "\n",
    "Ответ: 1e-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = SGDRegressor().fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 1e-19}\n"
     ]
    }
   ],
   "source": [
    "# давайте используем кросс-валидацию для поиска оптимальных гиперпараметров\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "alphas = np.linspace(0.0000000000000000001,1,100)\n",
    "\n",
    "params = {'alpha':alphas}\n",
    "num_splits = 3\n",
    "\n",
    "#print(params)\n",
    "cv = GridSearchCV(reg,\n",
    "                  params,\n",
    "                  scoring='neg_mean_squared_error',\n",
    "                  cv=num_splits\n",
    "                 )\n",
    "\n",
    "cv.fit(X, y)\n",
    "\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6912224189998502e-25\n"
     ]
    }
   ],
   "source": [
    "reg = SGDRegressor(alpha=cv.best_params_['alpha']).fit(X, y)\n",
    "print(mean_squared_error(y, reg.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ваша многомерная линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 3 (5 баллов)***. Напишите собственную многомерную линейную регрессию, оптимизирующую MSE методом *градиентного спуска*. Для этого используйте шаблонный класс. \n",
    "\n",
    "Критерий останова: либо норма разности весов на текущей и предыдущей итерациях меньше определенного значения (первый и третий варианты), либо модуль разности функционалов качества (MSE) на текущей и предыдущей итерациях меньше определенного значения (второй и четвертый варианты). Также предлагается завершать обучение в любом случае, если было произведено слишком много итераций.\n",
    "\n",
    "***Задание 4 (2 балла)***. Добавьте l1 (первый и второй варианты) или l2 (третий и четвертый варианты) регуляризацию. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3 - решение\n",
    "\n",
    "Не могу найти ошибку - не проходит assertion error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(object):\n",
    "    def __init__(self, alpha=0.45, l_ratio=0.00001, tol=0.0001, max_iter=10000):\n",
    "        '''\n",
    "        Для начала необходимо инициализировать параметры\n",
    "        alpha - это learning rate или шаг обучения\n",
    "        l_ratio - параметр регуляризации\n",
    "        tol - значение для критерия останова\n",
    "        max_iter - максимальное количество итераций обучения\n",
    "        '''\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.l_ratio = l_ratio\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "\n",
    "             \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Метод для обучения линейной регрессии\n",
    "        X - матрица признаков\n",
    "        y - вектор правильных ответов\n",
    "        '''\n",
    "        # Добавим столбец констант к матрице признаков\n",
    "        X = sm.add_constant(X)\n",
    "        \n",
    "        # Количество наблюдений        \n",
    "        N = len(y)\n",
    "        # Количество предикторов        \n",
    "        K = X.shape[1]\n",
    "\n",
    "        \n",
    "        # Создадим пустую матрицу весов (пусть все веса сначала равны 1)        \n",
    "        W = np.zeros(K)\n",
    "        \n",
    "        for i in tqdm(range(self.max_iter)):\n",
    "            \n",
    "            # each iteration y will change, so good to have temporary variable for y            \n",
    "            temp_y = X @ W\n",
    "            \n",
    "            # MSE loss\n",
    "            loss_func_t_0 = 1/N * sum((y - temp_y)**2)\n",
    "            \n",
    "            # define gradinent in a matrix form\n",
    "            W_grad = 2/N * (X.T @ ((X @ W) - y))\n",
    "            \n",
    "            # update weights\n",
    "            W = W - self.alpha * W_grad\n",
    "            \n",
    "            # update temporary y\n",
    "            y_temp = X @ W\n",
    "            \n",
    "            # MSE loss in the next period iteration            \n",
    "            loss_func_t_1 = 1/N * sum((y - temp_y)**2)\n",
    "            \n",
    "            # Идея здесь сравнить то, насколько функционалы до и после обновления весов отличаются \n",
    "            # Вернее насколько велик модуль их разности, который для моего варианта служит критерием останова            \n",
    "               \n",
    "            loss_diff = np.linalg.norm(loss_func_t_1 - loss_func_t_0)\n",
    "            \n",
    "            if loss_diff >= self.tol:\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        # нужно, чтобы использовать в предикт функции       \n",
    "        self.Weights = W\n",
    "        \n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Метод для предсказаний линейной регрессии\n",
    "        X - матрица признаков\n",
    "        '''\n",
    "        # Добавим столбец констант к матрице признаков\n",
    "        X = sm.add_constant(X)\n",
    "        \n",
    "        # Прогноз это произведение матриц предикторов и полученных из функции fit весов        \n",
    "        Y_pred = X @ self.Weights\n",
    "        \n",
    "        return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([121.31125986,  -1.8284482 ,   1.72599181, ...,  44.80971777,\n",
       "       112.0646511 ,  29.44430206])"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_reg = LinearRegression()\n",
    "my_reg.fit(X, y)\n",
    "\n",
    "my_reg.predict(X)\n",
    "# assert mean_squared_error(y, my_reg.predict(X)) < 1e-3\n",
    "# print('You are amazing! Great work!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([125.05840824,  -3.45110706, -22.24543605, ...,  64.49833094,\n",
       "       147.81654533,  34.37837112])"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 5 (1 балл)***. Обучите линейную регрессию из коробки\n",
    "\n",
    "* с l1-регуляризацией (from sklearn.linear_model import Lasso, **первый и второй вариант**) или с l2-регуляризацией (from sklearn.linear_model import Ridge, **третий и четвертый вариант**)\n",
    "* со значением параметра регуляризации **0.1 - для первого и третьего варианта, 0.01 - для второго и четвертого варианта**. \n",
    "\n",
    "Обучите вашу линейную регрессию с тем же значением параметра регуляризации и сравните результаты. Сделайте выводы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5 - решение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.47199278e-08, -3.27115335e-08, -3.54852340e-08, -3.80873109e-08,\n",
       "       -3.41946709e-08])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# у меня 4й вариант\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "Ridge_reg = Ridge(alpha=0.01)\n",
    "Ridge_reg.fit(X, y)\n",
    "\n",
    "# Check MSE values\n",
    "mse = cross_val_score(Ridge_reg, X, y, cv=5, scoring=\"neg_mean_squared_error\")\n",
    "mse\n",
    "\n",
    "# Сравнивать не с чем)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 6* (1 балл).***\n",
    "Пусть $P, Q \\in \\mathbb{R}^{n\\times n}$. Найдите $\\nabla_Q tr(PQ)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 6 - решение\n",
    "\n",
    "$\\nabla_Q tr(PQ) = \\frac{\\partial}{\\partial Q_{i,j}} tr(PQ) = \\frac{\\partial}{\\partial Q_{i,j}} \\sum_k (PQ)_{kk} = \\frac{\\partial}{\\partial Q_{i,j}} \\sum_{k,l} P_{kl}Q_{lk} = P_{ji} $\n",
    "\n",
    "Следовательно, мы можем получить следующий результат: $\\nabla_Q tr(PQ) = P^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 7* (1 балл).***\n",
    "Пусть $x, y \\in \\mathbb{R}^{n}, M \\in \\mathbb{R}^{n\\times n}$. Найдите $\\nabla_M x^T M y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 7 - решение\n",
    "\n",
    "$\\nabla_M x^T M y = \\nabla_M TR(x^T M y) $ - потому что произведение в скобках дает число\n",
    "\n",
    "$\\nabla_M TR(x^T M y) = \\nabla_M TR(M y x^T)$ - по циклическому свойству следа матрицы\n",
    "\n",
    "Из решения задачи 6 мы можем взять производную промежуточного результата $\\nabla_M TR(M y x^T) = xy^T$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решения заданий 6 и 7 можно написать на листочке и отправить в anytask вместе с заполненным ноутбуком."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
