{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QApBN7J5KcB"
   },
   "source": [
    "**Алексей Сек - БЭК182**\n",
    "\n",
    "2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aw_2A_r5kw_S"
   },
   "source": [
    "# **1 - Часть, скопированная из бейзлайна**\n",
    "\n",
    "Не очень хотелось изобретать велосипед с предобработкой данных, поэтому просто скопировал из бейзлайна. Все изменения и улучшения бейзлайна в следующей части!\n",
    "\n",
    "**Единственное изменение здесь - я выгружал данные со своего гугл диска, потому что dropbox у меня сначала выгружал данные, но потом почему-то перестал** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hJo7wgqV7cdv",
    "outputId": "0c4ea998-547c-457f-895a-8ed563513951"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting catboost==1.0.3\n",
      "  Downloading catboost-1.0.3-cp37-none-manylinux1_x86_64.whl (76.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 76.3 MB 1.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost==1.0.3) (1.19.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost==1.0.3) (1.4.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost==1.0.3) (1.15.0)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost==1.0.3) (1.1.5)\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost==1.0.3) (0.10.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost==1.0.3) (3.2.2)\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost==1.0.3) (4.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost==1.0.3) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost==1.0.3) (2018.9)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost==1.0.3) (3.0.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost==1.0.3) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost==1.0.3) (0.11.0)\n",
      "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost==1.0.3) (1.3.3)\n",
      "Installing collected packages: catboost\n",
      "Successfully installed catboost-1.0.3\n",
      "Collecting lightgbm==3.2.1\n",
      "  Downloading lightgbm-3.2.1-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 5.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from lightgbm==3.2.1) (0.37.0)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in /usr/local/lib/python3.7/dist-packages (from lightgbm==3.2.1) (1.0.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from lightgbm==3.2.1) (1.4.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from lightgbm==3.2.1) (1.19.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.22.0->lightgbm==3.2.1) (3.0.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.22.0->lightgbm==3.2.1) (1.1.0)\n",
      "Installing collected packages: lightgbm\n",
      "  Attempting uninstall: lightgbm\n",
      "    Found existing installation: lightgbm 2.2.3\n",
      "    Uninstalling lightgbm-2.2.3:\n",
      "      Successfully uninstalled lightgbm-2.2.3\n",
      "Successfully installed lightgbm-3.2.1\n",
      "Collecting cmake==3.22.0\n",
      "  Downloading cmake-3.22.0-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (21.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 21.8 MB 1.7 MB/s \n",
      "\u001b[?25hInstalling collected packages: cmake\n",
      "  Attempting uninstall: cmake\n",
      "    Found existing installation: cmake 3.12.0\n",
      "    Uninstalling cmake-3.12.0:\n",
      "      Successfully uninstalled cmake-3.12.0\n",
      "Successfully installed cmake-3.22.0\n",
      "Collecting xgboost==1.5.0\n",
      "  Downloading xgboost-1.5.0-py3-none-manylinux2014_x86_64.whl (173.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 173.5 MB 64 kB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost==1.5.0) (1.4.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost==1.5.0) (1.19.5)\n",
      "Installing collected packages: xgboost\n",
      "  Attempting uninstall: xgboost\n",
      "    Found existing installation: xgboost 0.90\n",
      "    Uninstalling xgboost-0.90:\n",
      "      Successfully uninstalled xgboost-0.90\n",
      "Successfully installed xgboost-1.5.0\n",
      "Collecting category_encoders\n",
      "  Downloading category_encoders-2.3.0-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 294 kB/s \n",
      "\u001b[?25hRequirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (0.5.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.19.5)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (0.10.2)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.4.1)\n",
      "Requirement already satisfied: pandas>=0.21.1 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.1->category_encoders) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.1->category_encoders) (2018.9)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.5.1->category_encoders) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.0.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.1.0)\n",
      "Installing collected packages: category-encoders\n",
      "Successfully installed category-encoders-2.3.0\n",
      "Collecting shap\n",
      "  Downloading shap-0.40.0-cp37-cp37m-manylinux2010_x86_64.whl (564 kB)\n",
      "\u001b[K     |████████████████████████████████| 564 kB 5.2 MB/s \n",
      "\u001b[?25hCollecting slicer==0.0.7\n",
      "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from shap) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from shap) (1.19.5)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from shap) (1.1.5)\n",
      "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.7/dist-packages (from shap) (4.62.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from shap) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from shap) (1.0.1)\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from shap) (0.51.2)\n",
      "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.7/dist-packages (from shap) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>20.9->shap) (3.0.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->shap) (57.4.0)\n",
      "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->shap) (0.34.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->shap) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->shap) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->shap) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->shap) (3.0.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->shap) (1.1.0)\n",
      "Installing collected packages: slicer, shap\n",
      "Successfully installed shap-0.40.0 slicer-0.0.7\n"
     ]
    }
   ],
   "source": [
    "#@title Pip install packages\n",
    "!pip install catboost==1.0.3\n",
    "!pip install lightgbm==3.2.1\n",
    "!pip install cmake==3.22.0 # без нее xgboost установится, но не будет импортироваться\n",
    "!pip install xgboost==1.5.0\n",
    "#@title Default title text\n",
    "!pip install category_encoders\n",
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qxc67XQT5im1",
    "outputId": "bb749663-11c0-415f-e2c4-c54887595599"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "#@title\n",
    "# Импортируем нужные библиотеки\n",
    "\n",
    "# Операции с данными\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Препроцессинг данных\n",
    "from category_encoders.leave_one_out import LeaveOneOutEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import f_classif, chi2, mutual_info_classif\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile\n",
    "\n",
    "# Модели, метрики, кросс-валидация\n",
    "import lightgbm as lgb\n",
    "import catboost as ctb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier \n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import Pool\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas.util.testing as tm\n",
    "\n",
    "# Прочее\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "# Визуализация\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rOv-8FKE_Ffk"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "# Код из бейзлайна\n",
    "def reduce_mem_usage(df):\n",
    "    NAlist = [] # Keeps track of columns that have missing values filled in. \n",
    "    for col in tqdm(df.columns):\n",
    "        if df[col].dtype != object:  # Exclude strings\n",
    "\n",
    "            # make variables for Int, max and min\n",
    "            IsInt = False\n",
    "            col_max_value = df[col].max()\n",
    "            col_min_value = df[col].min()\n",
    "\n",
    "            # Integer does not support NA, therefore, NA needs to be filled\n",
    "            if not np.isfinite(df[col]).all(): \n",
    "                NAlist.append(col)\n",
    "                df[col].fillna(col_min_value - 1, inplace=True)\n",
    "\n",
    "            # test if column can be converted to an integer\n",
    "            col_as_int = df[col].fillna(0).astype(np.int64)\n",
    "            diff = (df[col] - col_as_int)\n",
    "            diff = diff.sum()\n",
    "            if np.abs(diff) < 0.01:\n",
    "                IsInt = True\n",
    "\n",
    "            # Make Integer/unsigned Integer datatypes\n",
    "            if IsInt:\n",
    "                if col_min_value >= 0:\n",
    "                    if col_max_value < 255:\n",
    "                        df[col] = df[col].astype(np.uint8)\n",
    "                    elif col_max_value < 65535:\n",
    "                        df[col] = df[col].astype(np.uint16)\n",
    "                    elif col_max_value < 4294967295:\n",
    "                        df[col] = df[col].astype(np.uint32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.uint64)\n",
    "                else:\n",
    "                    if col_min_value > np.iinfo(np.int8).min and col_max_value < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif col_min_value > np.iinfo(np.int16).min and col_max_value < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif col_min_value > np.iinfo(np.int32).min and col_max_value < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "                    elif col_min_value > np.iinfo(np.int64).min and col_max_value < np.iinfo(np.int64).max:\n",
    "                        df[col] = df[col].astype(np.int64)    \n",
    "\n",
    "            # Make float datatypes 32 bit\n",
    "            else:\n",
    "                df[col] = df[col].astype(np.float32)\n",
    "\n",
    "    return df, NAlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lL21NoVtpBz8",
    "outputId": "042df0da-03ef-4f0e-ec3a-9bee107e7aaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "#@title\n",
    "from google.colab import drive \n",
    "drive.mount('/content/gdrive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YT4WXKHG82j6",
    "outputId": "809814e9-96b2-4e08-b319-04fe904c757e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 434/434 [00:55<00:00,  7.79it/s]\n",
      "100%|██████████| 433/433 [00:29<00:00, 14.90it/s] \n"
     ]
    }
   ],
   "source": [
    "#@title\n",
    "# Код из бейзлайна, по сути нужно же просто замерджить два датафрейма, ничего нового тут не придумать\n",
    "# INPUT_DIR = '.'\n",
    "\n",
    "\n",
    "train_transaction = pd.read_csv('/content/gdrive/MyDrive/train_transaction.csv')\n",
    "train_identity = pd.read_csv('/content/gdrive/MyDrive/train_identity.csv')\n",
    "test_transaction = pd.read_csv('/content/gdrive/MyDrive/test_transaction.csv')\n",
    "test_identity = pd.read_csv('/content/gdrive/MyDrive/test_identity.csv')\n",
    "# sample_submission = pd.read_csv(os.path.join(INPUT_DIR, 'sample_submission.csv'))\n",
    "\n",
    "df_train = train_transaction.merge(train_identity, how='left', on='TransactionID')\n",
    "del train_transaction, train_identity\n",
    "df_train, df_train_NAlist = reduce_mem_usage(df_train)\n",
    "\n",
    "df_test = test_transaction.merge(test_identity, how='left', on='TransactionID')\n",
    "del test_transaction, test_identity\n",
    "df_test, df_test_NAlist = reduce_mem_usage(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VWrl_4sM8AUW",
    "outputId": "e734bf4e-421a-44ec-de9c-9ff635e2bc52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data in train: 0.00000%\n",
      "Missing data in test: 0.00000%\n"
     ]
    }
   ],
   "source": [
    "#@title\n",
    "# Код из бейзлайна - Заполним пропуски\n",
    "for col in df_train.columns.drop('isFraud'):\n",
    "    if df_train[col].dtype == 'O':\n",
    "        df_train[col] = df_train[col].fillna('unseen_category')\n",
    "        df_test[col] = df_test[col].fillna('unseen_category')\n",
    "    else:\n",
    "        df_train[col] = df_train[col].fillna(-1)\n",
    "        df_test[col] = df_test[col].fillna(-1)\n",
    "\n",
    "print('Missing data in train: {:.5f}%'.format(df_train.isnull().sum().sum() / (df_train.shape[0] * df_train.shape[1]) * 100))\n",
    "print('Missing data in test: {:.5f}%'.format(df_test.isnull().sum().sum() / (df_test.shape[0] * df_test.shape[1]) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-vAkcaX3PSmg",
    "outputId": "f54da8ec-9ebe-4d2c-a6aa-240364cd80fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 433/433 [00:13<00:00, 31.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 417559 entries, 0 to 417558\n",
      "Columns: 434 entries, TransactionID to DeviceInfo\n",
      "dtypes: category(31), float32(80), int16(7), int8(9), uint16(40), uint32(3), uint8(264)\n",
      "memory usage: 294.8 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 172981 entries, 0 to 172980\n",
      "Columns: 433 entries, TransactionID to DeviceInfo\n",
      "dtypes: category(31), float32(78), int16(6), int8(9), uint16(57), uint32(3), uint8(249)\n",
      "memory usage: 123.6 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title\n",
    "# Код из бейзлайна - Закодируем категориальные признаки\n",
    "for col in tqdm(df_train.columns.drop('isFraud')):\n",
    "    if df_train[col].dtype == 'O':\n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(df_train[col]) + list(df_test[col]))\n",
    "        df_train[col] = le.transform(df_train[col])\n",
    "        df_test[col] = le.transform(df_test[col])\n",
    "        \n",
    "        df_train[col] = df_train[col].astype('category')\n",
    "        df_test[col] = df_test[col].astype('category')\n",
    "\n",
    "df_train.info(), df_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCmVgmGU6r2a"
   },
   "source": [
    "# **2 - Здесь начинается моя часть**\n",
    "\n",
    "**В моем решении есть два отличия от бейзлайна:**\n",
    "1. Удаление всех признаков V (V1 - V340)\n",
    "2. Использование gridsearch и randomsearch cv для поиска оптимальных параметров\n",
    "\n",
    "**В первом пункте моя логика была следующей:** после ознакомления с baseline eda, мне показалось странным, что в нем никак не затрагивались столбцы V, в отличие от, например, столбцов C или D. Поэтому я решил посмотреть, какую информацию несут в себе признаки V, оказалось, что это \"сгенерированные вручную признаки на основе ранжирования, подсчета и других методов\". Учитывая, что довольно непрозрачно то, каким образом были посчитаны эти признаки, а также то, что эти признаки могут приводить к переобучению и/или практически повторять другие признаки, что привело бы к снижению качества на тесте - я решил попробовать удалить эти признаки вовсе и обучить модель c дефолтными гиперпараметрами\n",
    "\n",
    "**Во втором пункте моя логика была сдедующей:** если даже с дефолтными гиперпараметрами, качество повысилось (стало чуть больше 89%), то почему бы не попробовать найти оптимальные гиперпараметры? К сожалению, полный перебор через gridsearch занимает слишком много времени, у меня просто получалось, что google colab disconnected, даже если включить caffeinate на компьютере и не давать ему уснуть. Поэтому я решил сделать случайный частичный перебор гиперпараметров - randomsearch cross-validation. Я запускал кросс-валидацию несколько раз и почти всегда некоторые параметры были одинаковыми, а именно: 'colsample_bytree': 0.325,\n",
    " 'learning_rate': 0.025, поэтому можно взять эти параметры фиксированными и сделать полный перебор для 'n_estimators', при фиксированном 'num_leaves'. (еще до финальной кросс-валидации этих двух гиперапарметров я загружал несколько раз с разными их значениями после random search, в целом около 91% ROC-AUC получалось на public test)\n",
    "\n",
    "**Идеи для улучшения:**\n",
    "1. Кроме удаления признаков V - можно еще глубже копнуть в EDA, например, и попробовать улучшить данные, на которых мы обучаемся: либо удалить ненужные признаки, либо сгенерировать дополнтиельные. \n",
    "2. Сделать полный перебор по всем гиперпараметрам + делать не 2-fold, а 3-fold или 5-fold CV для этого. Но, к сожалению, это займет слишком много времени, а я не хочу так долго ждать)\n",
    "3. Обучить еще xgboost и catboost, после этого сделать блендинг всех алгоритмов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhgKQ1Vnmwda"
   },
   "source": [
    "**Пункт 1 - удаление не самых полезных фичей**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "XXvLdsDke4bk"
   },
   "outputs": [],
   "source": [
    "# Удалим не очень полезные для обучения столбцы из датасета\n",
    "# ID точно не фича, DT можно привести в какой-то более правильный вид, но не знаю, как лучше его использовать\n",
    "# isFraud вообще таргет, а не фича\n",
    "remove = ['TransactionID','TransactionDT', 'isFraud']\n",
    "\n",
    "# Удалим столбцы V, потому что так будет быстрее обучение, а эти столбцы не самые важные\n",
    "for i in range(1,340):\n",
    "  remove.append('V'+str(i))\n",
    "\n",
    "# Cписок отобранных фичей\n",
    "selected_features = []\n",
    "for column in df_train.columns.values:\n",
    "   if column not in remove:\n",
    "     selected_features.append(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hc3jRYWem4n2"
   },
   "source": [
    "**Пункт 2 - обучение модели** - дефолтные гиперпараметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "2CKz0vk7FF-K"
   },
   "outputs": [],
   "source": [
    "# Вытащим данные для обучения модели\n",
    "X,y = df_train[selected_features], df_train['isFraud']\n",
    "\n",
    "# Разделим на train и test \n",
    "X_test = df_test[selected_features]\n",
    "X_y_train = lgb.Dataset(X, label=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-TgEO-mIFGCW",
    "outputId": "84fee6cc-1b49-4720-dddb-f05acaabf7d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 14721, number of negative: 402838\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.231308 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10857\n",
      "[LightGBM] [Info] Number of data points in the train set: 417559, number of used features: 92\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035255 -> initscore=-3.309259\n",
      "[LightGBM] [Info] Start training from score -3.309259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                                                           3404559\n",
       "1                                                           3404560\n",
       "2                                                           3404561\n",
       "3                                                           3404562\n",
       "4                                                           3404563\n",
       "                                        ...                        \n",
       "172977                                                      3577536\n",
       "172978                                                      3577537\n",
       "172979                                                      3577538\n",
       "172980                                                      3577539\n",
       "optimal_params    [0.006195203184603711, 0.08011748680711864, 0....\n",
       "Name: TransactionID, Length: 172982, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Некоторые важные параметры, которые стоит изменить с дефолтных, это важно идейно для задачи скорее\n",
    "lgb_params = {}\n",
    "lgb_params['objective'] ='binary' # потому что у нас бинарная классификация\n",
    "lgb_params['n_jobs'] = -1 # чтобы использовались все ядра\n",
    "\n",
    "estimator = lgb.train(\n",
    "                lgb_params,\n",
    "                X_y_train)\n",
    "\n",
    "answer = df_test['TransactionID']\n",
    "answer['optimal_params'] = estimator.predict(X_test)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "SOFZYtbYnfU0"
   },
   "outputs": [],
   "source": [
    "# Приведем ответ в нужный формат\n",
    "res = pd.DataFrame(answer)\n",
    "list_res = res.iloc[-1,0]\n",
    "res = res.iloc[:-1,:]\n",
    "res['isFraud'] = list_res\n",
    "\n",
    "# Сохраним ответ в csv\n",
    "res.to_csv('lgbm_default.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JtfyUl-lphK-"
   },
   "source": [
    "**Пункт 2 - обучение модели** - подбор оптимальных гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pzwsDpI001v0",
    "outputId": "2bec7212-ffc1-4a1f-e444-83dd6c686ace"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 500}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Параметры для grid я выбирал методом тыка, просто нельзя перебрать все-все\n",
    "# Поэтому я брал те, которые каждутся мне адекватными\n",
    "# Попробуем только количество estimators выбрать отдельно\n",
    "# Это я чисто затестил, работает ли все в коде или нет, чтобы долго не ждать\n",
    "\n",
    "grid_params = {\n",
    "# 'learning_rate' : [0.01, 0.025, 0.05],\n",
    "# 'num_leaves': np.arange(10,100,20),\n",
    "# 'colsample_bytree': np.linspace(0.1,1,5),\n",
    "'n_estimators': np.arange(100,600,100),\n",
    " }\n",
    "\n",
    "model = LGBMClassifier(objective='binary', n_jobs=-1, metric='roc_auc', learning_rate=0.05)\n",
    "\n",
    "# Используем все ядра для кросс-валидации, скоринг - как в задаче, verbose, чтобы видеть прогресс\n",
    "grid = GridSearchCV(model, grid_params, scoring='roc_auc', n_jobs=-1, verbose=2, cv=2)\n",
    "grid.fit(X, y)\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vFlO6itGLVGo",
    "outputId": "6bea74a6-805a-4eb2-cd0b-715c339af755"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.325,\n",
       " 'learning_rate': 0.025,\n",
       " 'n_estimators': 500,\n",
       " 'num_leaves': 90}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Тут я взял всего 20 n_iter, чтобы было быстро\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Выберем некоторые параметры для кросс-валидации\n",
    "grid_params = {\n",
    "'learning_rate' : [0.01, 0.025, 0.05],\n",
    "'num_leaves': np.arange(10,100,20),\n",
    "'colsample_bytree': np.linspace(0.1,1,5),\n",
    "'n_estimators': np.arange(100,600,100),\n",
    "}\n",
    "\n",
    "model = LGBMClassifier(objective='binary', n_jobs=-1, metric='roc_auc')\n",
    "\n",
    "# Используем все ядра для кросс-валидации, скоринг - как в задаче, verbose, чтобы видеть прогресс\n",
    "grid = RandomizedSearchCV(model, grid_params, n_iter=100, scoring='roc_auc', n_jobs=-1, verbose=2, cv=2)\n",
    "grid.fit(X, y)\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "igs7b18-L5BM",
    "outputId": "4a0ad9ed-aafe-4395-bac9-be78c9d128d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 300 candidates, totalling 600 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.325,\n",
       " 'learning_rate': 0.025,\n",
       " 'n_estimators': 400,\n",
       " 'num_leaves': 70}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Еще разок сделаем randomsearch (код ранился больше шести часов для 200 фитов)\n",
    "# Тут я взял всего 300 n_iter, чтобы посмотреть, сильно ли улучшится качество с такими параметрами\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Выберем некоторые параметры для кросс-валидации\n",
    "grid_params = {\n",
    "'learning_rate' : [0.01, 0.025, 0.05],\n",
    "'num_leaves': np.arange(10,100,20),\n",
    "'colsample_bytree': np.linspace(0.1,1,5),\n",
    "'n_estimators': np.arange(100,600,100)\n",
    "}\n",
    "\n",
    "model = LGBMClassifier(objective='binary', n_jobs=-1, metric='roc_auc')\n",
    "\n",
    "# Используем все ядра для кросс-валидации, скоринг - как в задаче, verbose, чтобы видеть прогресс\n",
    "grid = RandomizedSearchCV(model, grid_params, n_iter=300, scoring='roc_auc', n_jobs=-1, verbose=2, cv=2)\n",
    "grid.fit(X, y)\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v4Y66G0tWUtU",
    "outputId": "a0813ed0-a137-48e9-ec2e-1a209e9afce4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.325,\n",
       " 'learning_rate': 0.025,\n",
       " 'n_estimators': 300,\n",
       " 'num_leaves': 200}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Еще разок сделаем randomsearch (код ранился больше двух часов для 200 фитов)\n",
    "# Тут я взял всего 100 n_iter, при этом немного сменил grid_params для num_leaves и n_estimators\n",
    "# Решил поменять, потому что grid все равно задавался методом тыка\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Выберем некоторые параметры для кросс-валидации\n",
    "grid_params = {\n",
    "'learning_rate' : [0.01, 0.025, 0.05],\n",
    "'num_leaves': np.arange(50,300,50),\n",
    "'colsample_bytree': np.linspace(0.1,1,5),\n",
    "'n_estimators': np.arange(200,700,100)\n",
    "}\n",
    "\n",
    "model = LGBMClassifier(objective='binary', n_jobs=-1, metric='roc_auc')\n",
    "\n",
    "# Используем все ядра для кросс-валидации, скоринг - как в задаче, verbose, чтобы видеть прогресс\n",
    "grid = RandomizedSearchCV(model, grid_params, n_iter=100, scoring='roc_auc', n_jobs=-1, verbose=2, cv=2)\n",
    "grid.fit(X, y)\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TgO6vFp0WUvk",
    "outputId": "47c1948f-8801-4c32-da9e-c543fa513c26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 14721, number of negative: 402838\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.186996 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10857\n",
      "[LightGBM] [Info] Number of data points in the train set: 417559, number of used features: 92\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035255 -> initscore=-3.309259\n",
      "[LightGBM] [Info] Start training from score -3.309259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                                                           3404559\n",
       "1                                                           3404560\n",
       "2                                                           3404561\n",
       "3                                                           3404562\n",
       "4                                                           3404563\n",
       "                                        ...                        \n",
       "172977                                                      3577536\n",
       "172978                                                      3577537\n",
       "172979                                                      3577538\n",
       "172980                                                      3577539\n",
       "optimal_params    [0.004696759725603189, 0.0543864558501996, 0.0...\n",
       "Name: TransactionID, Length: 172982, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Возьмем оптимальные параметры из кросс-валидации\n",
    "\n",
    "X_test = df_test[selected_features]\n",
    "X_y_train = lgb.Dataset(X, label=y)\n",
    "\n",
    "lgb_params = {}\n",
    "lgb_params['n_estimators'] = 300 # из кросс-валидации\n",
    "lgb_params['objective'] ='binary' # потому что у нас бинарная классификация\n",
    "lgb_params['n_jobs'] = -1 # чтобы использовались все ядра\n",
    "lgb_params['metric'] = 'roc_auc' # потому что это наша метрика в задаче\n",
    "lgb_params['colsample_bytree'] = 0.325\n",
    "lgb_params['learning_rate'] = 0.025\n",
    "lgb_params['num_leaves'] = 200\n",
    "\n",
    "estimator = lgb.train(\n",
    "                lgb_params,\n",
    "                X_y_train)\n",
    "\n",
    "answer = df_test['TransactionID']\n",
    "answer['optimal_params'] = estimator.predict(X_test)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "PW3UnfLaKplU",
    "outputId": "5d15decd-eae6-443a-b7d7-a322f600f7e1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>isFraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3404559</td>\n",
       "      <td>0.004697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3404560</td>\n",
       "      <td>0.054386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3404561</td>\n",
       "      <td>0.018074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3404562</td>\n",
       "      <td>0.013530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3404563</td>\n",
       "      <td>0.303040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172976</th>\n",
       "      <td>3577535</td>\n",
       "      <td>0.013198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172977</th>\n",
       "      <td>3577536</td>\n",
       "      <td>0.011908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172978</th>\n",
       "      <td>3577537</td>\n",
       "      <td>0.004879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172979</th>\n",
       "      <td>3577538</td>\n",
       "      <td>0.039513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172980</th>\n",
       "      <td>3577539</td>\n",
       "      <td>0.020495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>172981 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       TransactionID   isFraud\n",
       "0            3404559  0.004697\n",
       "1            3404560  0.054386\n",
       "2            3404561  0.018074\n",
       "3            3404562  0.013530\n",
       "4            3404563  0.303040\n",
       "...              ...       ...\n",
       "172976       3577535  0.013198\n",
       "172977       3577536  0.011908\n",
       "172978       3577537  0.004879\n",
       "172979       3577538  0.039513\n",
       "172980       3577539  0.020495\n",
       "\n",
       "[172981 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Приведем ответ в нужный формат\n",
    "res = pd.DataFrame(answer)\n",
    "list_res = res.iloc[-1,0]\n",
    "res = res.iloc[:-1,:]\n",
    "res['isFraud'] = list_res\n",
    "\n",
    "# Сохраним ответ в csv\n",
    "res.to_csv('lgbm_optimal2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XURDr4MoqqSD"
   },
   "source": [
    "**Заметим, что после изменения grid, количество оптимальных листьев очень сильно увеличилось, но при этом снизилось количество n_estimators, давайте попробуем такую модель и взять.**\n",
    "С такими гиперпараметрами - на public_test, модель получилас roc-auc score 0.91595\n",
    "\n",
    "Так как 'colsample_bytree': 0.325 и 'learning_rate': 0.025, получились такими во всех randomsearch - давайте их зафиксируем и сделаем полный перебор только по n_estimators от 100 до 1000 с шагом 100, при этом num_leaves оставим 200, как в предыдущем шаге (если честно, надо было их тоже gridsearchcv оптимизировать, но мне не хватает уже терпения ждать, очень медленно идет поиск оптимальных гиперпараметров)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JHd8z99fb_3N",
    "outputId": "b0e6578c-59d5-4897-e544-5ccd6ca4124c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 400}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Еще разок сделаем randomsearch (код ранился больше двух часов для 200 фитов)\n",
    "# Тут я взял всего 100 n_iter, при этом немного сменил grid_params для num_leaves и n_estimators\n",
    "# Решил поменять, потому что grid все равно задавался методом тыка\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Выберем некоторые параметры для кросс-валидации\n",
    "grid_params = {\n",
    "'n_estimators': np.arange(100,1000,100)\n",
    "}\n",
    "\n",
    "model = LGBMClassifier(objective='binary', n_jobs=-1, metric='roc_auc', learning_rate=0.025, num_leaves=200, colsample_bytree=0.325)\n",
    "\n",
    "# Используем все ядра для кросс-валидации, скоринг - как в задаче, также здесь возьмем 3-fold, а не 2-fold cv\n",
    "grid = GridSearchCV(model, grid_params, scoring='roc_auc', n_jobs=-1, verbose=2, cv=3)\n",
    "grid.fit(X, y)\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EW4TKUMLb_5i",
    "outputId": "7395e294-a0da-48f5-b533-38cba12c0452"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 14721, number of negative: 402838\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.211006 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10857\n",
      "[LightGBM] [Info] Number of data points in the train set: 417559, number of used features: 92\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035255 -> initscore=-3.309259\n",
      "[LightGBM] [Info] Start training from score -3.309259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                                                           3404559\n",
       "1                                                           3404560\n",
       "2                                                           3404561\n",
       "3                                                           3404562\n",
       "4                                                           3404563\n",
       "                                        ...                        \n",
       "172977                                                      3577536\n",
       "172978                                                      3577537\n",
       "172979                                                      3577538\n",
       "172980                                                      3577539\n",
       "optimal_params    [0.0030580552689698777, 0.045208885566753086, ...\n",
       "Name: TransactionID, Length: 172982, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Возьмем оптимальные параметры из кросс-валидации\n",
    "\n",
    "X_test = df_test[selected_features]\n",
    "X_y_train = lgb.Dataset(X, label=y)\n",
    "\n",
    "lgb_params = {}\n",
    "lgb_params['n_estimators'] = 400 # из кросс-валидации\n",
    "lgb_params['objective'] ='binary' # потому что у нас бинарная классификация\n",
    "lgb_params['n_jobs'] = -1 # чтобы использовались все ядра\n",
    "lgb_params['metric'] = 'roc_auc' # потому что это наша метрика в задаче\n",
    "lgb_params['colsample_bytree'] = 0.325\n",
    "lgb_params['learning_rate'] = 0.025\n",
    "lgb_params['num_leaves'] = 200\n",
    "\n",
    "estimator = lgb.train(\n",
    "                lgb_params,\n",
    "                X_y_train)\n",
    "\n",
    "answer = df_test['TransactionID']\n",
    "answer['optimal_params'] = estimator.predict(X_test)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Pr46cIJ2Kpnu"
   },
   "outputs": [],
   "source": [
    "# Приведем ответ в нужный формат\n",
    "res = pd.DataFrame(answer)\n",
    "list_res = res.iloc[-1,0]\n",
    "res = res.iloc[:-1,:]\n",
    "res['isFraud'] = list_res\n",
    "\n",
    "# Сохраним ответ в csv\n",
    "res.to_csv('lgbm_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xc3jkMQyvqVj"
   },
   "source": [
    "**Финальный score на public_test:** 0.91733\n",
    "\n",
    "**Почему у меня есть AUC-ROC 0.91752, хотя в финальной версии немного ниже?** Это довольно забавный момент - после кросс-валидации я несколько раз руками решил поставить разные гиперпараметры и если взять 500 n_estimators, а не 400, как после CV на train_data, на test_data результат получается лучше"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Kaggle_HSE_Sek.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
